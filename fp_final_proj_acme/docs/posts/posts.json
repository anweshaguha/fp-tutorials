[
  {
    "path": "posts/2022-06-04-building-blocks-o-functional-programming/",
    "title": "Building Blocks of Functional Programming",
    "description": "Walk through of basic data structure explorations using map",
    "author": [
      {
        "name": "Cassie Malcolm",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-06-04",
    "categories": [],
    "contents": "\n\nHello, and welcome - I assume you are here because you want to learn more about data science and in particular functional programming. This blog is designed to help you proceed on your data science path by providing an introduction to the basic building blocks of functional programming. Weâ€™ll be working with a freely available data set on LegosÂ©, which you can find here https://www.kaggle.com/datasets/rtatman/lego-database?resource=download.\nLetâ€™s start by loading the following packages: purrr, tidyverse, and repurrrsive. Take a moment to use ?package name to find out what each package does.\nNext we need to set up a directory for the csv files that we will be using.\n\n\nBBdata_dir <- here(\"Lego\")\n\n\n\nIâ€™ve already prepped two of the files for our use today by making sure there is an ID column that matches. We can see what the csv files are by listing them and using regexp to exclude any non-csv files.\n\n\nBBcsv <- fs::dir_ls(BBdata_dir, regexp = \"\\\\.csv$\")\n\nBBcsv\n\n\n/Users/aguha/Documents/r_projects/edld653-22/fp_collab/fp-tutorials/fp_final_proj_acme/Lego/colors2.csv\n/Users/aguha/Documents/r_projects/edld653-22/fp_collab/fp-tutorials/fp_final_proj_acme/Lego/inventory_parts.csv\n\nNext weâ€™ll use one of the base functions in R, lapply, to return a list. The lapply function is one of the least restrictive in terms of output within the apply family of functions. Notice that our two csv files are being combined into one vector based on the color_id column. The function reduce from the purrr package allows us to join the csv files on a key, which in this case is â€œcolor_idâ€.\n\n\nBBlist <- BBcsv %>% \n  lapply(read_csv) %>%\n  reduce(full_join, by = \"color_id\")\n\nhead(BBlist)\n\n\n# A tibble: 6 Ã— 9\n   ...1 color_id name    rgb   is_trans inventory_id part_num quantity\n  <dbl>    <dbl> <chr>   <chr> <lgl>           <dbl> <chr>       <dbl>\n1     1       -1 Unknown 0033â€¦ FALSE              80 belvfaiâ€¦        1\n2     1       -1 Unknown 0033â€¦ FALSE              80 belvfemâ€¦        1\n3     1       -1 Unknown 0033â€¦ FALSE              80 belvmalâ€¦        1\n4     1       -1 Unknown 0033â€¦ FALSE             214 fab6e           1\n5     1       -1 Unknown 0033â€¦ FALSE             250 belvfaiâ€¦        1\n6     1       -1 Unknown 0033â€¦ FALSE             250 belvfemâ€¦        1\n# â€¦ with 1 more variable: is_spare <lgl>\n\nWe can confirm that this is a list with the typeof function. Lists can be very helpful as they are vectors (but NOT atomic ðŸ¤¯ ones!) that allow each element to be of a different type (i.e., integer, double, logical, character).\n\n\ntypeof(BBlist)\n\n\n[1] \"list\"\n\nIf we want to know the structure of our list we can use the below function to see our listâ€™s elements.\n\n\nstr(BBlist)\n\n\nspec_tbl_df [580,255 Ã— 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ ...1        : num [1:580255] 1 1 1 1 1 1 1 1 1 1 ...\n $ color_id    : num [1:580255] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\n $ name        : chr [1:580255] \"Unknown\" \"Unknown\" \"Unknown\" \"Unknown\" ...\n $ rgb         : chr [1:580255] \"0033B2\" \"0033B2\" \"0033B2\" \"0033B2\" ...\n $ is_trans    : logi [1:580255] FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ inventory_id: num [1:580255] 80 80 80 214 250 250 250 250 382 382 ...\n $ part_num    : chr [1:580255] \"belvfair6\" \"belvfem26\" \"belvmale13\" \"fab6e\" ...\n $ quantity    : num [1:580255] 1 1 1 1 1 1 1 1 1 1 ...\n $ is_spare    : logi [1:580255] FALSE FALSE FALSE FALSE FALSE FALSE ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   ...1 = col_double(),\n  ..   color_id = col_double(),\n  ..   name = col_character(),\n  ..   rgb = col_character(),\n  ..   is_trans = col_logical()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\nWhat if we want to look at a particular row and column [row, column] of our list?\n\n\nBBlist[5, 2]\n\n\n# A tibble: 1 Ã— 1\n  color_id\n     <dbl>\n1       -1\n\nLetâ€™s check and see if we have any missing values in our list.\n\n\nany(is.na(BBlist))\n\n\n[1] TRUE\n\nSince there are NAs in our list we can determine their position with the following code.\n\n\nwhich(is.na(BBlist))\n\n\n [1] 3286912 3479282 3479283 3479285 3867167 4059537 4059538 4059540\n [9] 4447422 4639792 4639793 4639795 5027677 5220047 5220048 5220050\n\nNow letâ€™s determine an artificial mean for our data set. To do this weâ€™ll use our first functional, which is the map_chr function of the purrr package. A functional is basically a type of function whose input is also a function and whose output is a vector. In this case the function is mean, which will return the mean of each inventory_id. Using map_chr ensures that the output in the new column is of type character. Note that members of the map family return a dataframe.\n\n\nBBdf <- BBlist %>% \n  mutate(average = map_chr(inventory_id, mean))\n\nhead(BBdf)\n\n\n# A tibble: 6 Ã— 10\n   ...1 color_id name    rgb   is_trans inventory_id part_num quantity\n  <dbl>    <dbl> <chr>   <chr> <lgl>           <dbl> <chr>       <dbl>\n1     1       -1 Unknown 0033â€¦ FALSE              80 belvfaiâ€¦        1\n2     1       -1 Unknown 0033â€¦ FALSE              80 belvfemâ€¦        1\n3     1       -1 Unknown 0033â€¦ FALSE              80 belvmalâ€¦        1\n4     1       -1 Unknown 0033â€¦ FALSE             214 fab6e           1\n5     1       -1 Unknown 0033â€¦ FALSE             250 belvfaiâ€¦        1\n6     1       -1 Unknown 0033â€¦ FALSE             250 belvfemâ€¦        1\n# â€¦ with 2 more variables: is_spare <lgl>, average <chr>\n\nHow about determining the unique amount of each variable in our dataframe? We can use another functional! Note that purrr uses less code by using the ~ symbol to take the place of function(x), which is often used with lapply, with .x as the placeholder for the thing being looped over. In this case each variable is being looped over.\n\n\nmap(BBdf, ~length(unique(.x)))\n\n\n$...1\n[1] 135\n\n$color_id\n[1] 135\n\n$name\n[1] 135\n\n$rgb\n[1] 124\n\n$is_trans\n[1] 2\n\n$inventory_id\n[1] 10725\n\n$part_num\n[1] 23132\n\n$quantity\n[1] 240\n\n$is_spare\n[1] 3\n\n$average\n[1] 10725\n\nMost of the time we have to look at individual building blocks before we can proceed to solving larger functional programming problems. Letâ€™s start by determining the the number of unique times each color appears. The n column in count_colors dataframe below shows on how many different parts a color appears.\n\n\ncount_colors <- count(BBdf, name)\n\nhead(count_colors)\n\n\n# A tibble: 6 Ã— 2\n  name              n\n  <chr>         <int>\n1 [No Color]     2245\n2 Aqua             96\n3 Black        115176\n4 Blue          29857\n5 Blue-Violet      67\n6 Bright Green   1695\n\nSecond, we can use the tally function to find the total number of parts when you consider color as a part variation. We can use this number later to test if the first function we are about to write is working correctly.\n\n\ncount_colors %>%\n  tally(n)\n\n\n# A tibble: 1 Ã— 1\n       n\n   <int>\n1 580255\n\nOur first function, super exciting! Here is an overview of the parts of the below function, which is built to return a percentage.\nThe functionâ€™s name is my_function Percentage is an object that is later printed with its second usage An argument x is found in the formal component of (x) The body of the function is found between the {} The function format allows us to remove scientific notation with = F The function round allows our output to be limited to two decimal places\n\n\nmy_function <- function(x) {Percentage = (x/sum(x))*100; format(round(Percentage, 2), scientific = F)}\n\nmy_function(count_colors$n)\n\n\n  [1] \" 0.39\" \" 0.02\" \"19.85\" \" 5.15\" \" 0.01\" \" 0.29\" \" 0.06\" \" 0.34\"\n  [9] \" 0.05\" \" 0.25\" \" 0.60\" \" 0.00\" \" 0.00\" \" 0.00\" \" 0.13\" \" 0.00\"\n [17] \" 0.00\" \" 0.09\" \" 0.01\" \" 0.08\" \" 0.65\" \" 0.00\" \" 7.57\" \" 0.34\"\n [25] \" 0.01\" \" 1.31\" \" 0.34\" \" 0.24\" \" 0.26\" \" 0.29\" \" 0.76\" \" 0.62\"\n [33] \" 0.06\" \" 0.01\" \" 0.01\" \" 0.01\" \" 0.66\" \" 0.01\" \" 0.00\" \" 0.01\"\n [41] \" 0.00\" \" 0.00\" \" 0.00\" \" 0.01\" \" 0.00\" \" 0.01\" \" 2.04\" \" 0.06\"\n [49] \" 0.05\" \" 0.01\" \" 9.53\" \" 0.31\" \" 4.37\" \" 0.01\" \" 0.00\" \" 0.00\"\n [57] \" 0.00\" \" 0.00\" \" 0.00\" \" 0.00\" \" 0.01\" \" 0.02\" \" 0.85\" \" 0.03\"\n [65] \" 0.20\" \" 0.27\" \" 0.35\" \" 0.20\" \" 0.00\" \" 0.02\" \" 0.14\" \" 0.01\"\n [73] \" 0.03\" \" 0.00\" \" 0.01\" \" 0.04\" \" 0.00\" \" 0.14\" \" 0.02\" \" 0.12\"\n [81] \" 1.06\" \" 0.21\" \" 0.70\" \" 0.00\" \" 0.25\" \" 0.00\" \" 0.00\" \" 0.06\"\n [89] \" 0.04\" \" 8.65\" \" 2.44\" \" 0.00\" \" 0.05\" \" 0.00\" \" 0.00\" \" 0.12\"\n [97] \" 0.17\" \" 0.00\" \" 0.01\" \" 0.00\" \" 0.00\" \" 0.00\" \" 0.01\" \" 0.00\"\n[105] \" 2.36\" \" 0.00\" \" 0.00\" \" 0.00\" \" 0.38\" \" 0.00\" \" 0.08\" \" 1.32\"\n[113] \" 0.45\" \" 0.14\" \" 0.34\" \" 0.71\" \" 0.00\" \" 0.05\" \" 0.37\" \" 0.34\"\n[121] \" 0.00\" \" 0.41\" \" 0.02\" \" 0.07\" \" 0.99\" \" 0.01\" \" 0.57\" \" 0.02\"\n[129] \" 0.02\" \" 0.00\" \" 0.00\" \" 0.01\" \"11.47\" \" 6.69\" \" 0.03\"\n\nIf we want to check that our function is working correctly we can use a data point (letâ€™s check the second data point for color Aqua) and manually determine the corresponding percentage.\n\n\n(96/580255)*100\n\n\n[1] 0.01654445\n\nOur function worked! The above number when rounded is 0.02, which is the second percentage.\nThat wraps up our introduction to functional programming! In the next parts of the blog youâ€™ll learn more about functions and how to perform more complex functional programming.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-06-06T13:10:52-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-04-creating-general-data-simulation-tools/",
    "title": "Creating General Data Simulation Tools",
    "description": "Create your own functions to easily simulate datasets!",
    "author": [
      {
        "name": "Errol Kaylor",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-06-04",
    "categories": [],
    "contents": "\nOutline\nOverall goal: demonstrate ways to generate datasets for classroom\nuse!\nPart 1: Mocking data based on existing data set (of\nnumerical/character data) - Load in data, and produce descriptive stats.\n- Create a list of the columns/data types that are you are looking to\nmock up. - Populate the list with mocked data, depending on the types. -\nLoop through the process, depending on # of data sets needed. -\nDemonstrate function that walks them through the process â€“ Simulating\ndifferent types of data! - Mocking numerical data â€“ Distribution\nfunctions, lists - Mocking character data â€“ Use of stringi for character\nstrings â€“ Generate strings of specific length/format - Mocking logical\ndata â€“ Mock survey data\n\n\nlibrary(palmerpenguins)\nlibrary(stats)\nlibrary(psych)\nlibrary(ggplot2)\nlibrary(tidyverse)\npenguins\n\n\n# A tibble: 344 x 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm\n   <fct>   <fct>              <dbl>         <dbl>             <int>\n 1 Adelie  Torgersen           39.1          18.7               181\n 2 Adelie  Torgersen           39.5          17.4               186\n 3 Adelie  Torgersen           40.3          18                 195\n 4 Adelie  Torgersen           NA            NA                  NA\n 5 Adelie  Torgersen           36.7          19.3               193\n 6 Adelie  Torgersen           39.3          20.6               190\n 7 Adelie  Torgersen           38.9          17.8               181\n 8 Adelie  Torgersen           39.2          19.6               195\n 9 Adelie  Torgersen           34.1          18.1               193\n10 Adelie  Torgersen           42            20.2               190\n# ... with 334 more rows, and 3 more variables: body_mass_g <int>,\n#   sex <fct>, year <int>\n\n#first function, tell us descriptives, and show us some boxplots?\nstr(penguins)\n\n\ntibble [344 x 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\nspecies_tables <- describeBy(penguins,group=\"species\")\n\nspecies_tables\n\n\n\n Descriptive statistics by group \nspecies: Adelie\n                  vars   n    mean     sd median trimmed    mad\nspecies*             1 152    1.00   0.00    1.0    1.00   0.00\nisland*              2 152    2.05   0.80    2.0    2.07   1.48\nbill_length_mm       3 151   38.79   2.66   38.8   38.75   2.97\nbill_depth_mm        4 151   18.35   1.22   18.4   18.29   1.19\nflipper_length_mm    5 151  189.95   6.54  190.0  189.93   7.41\nbody_mass_g          6 151 3700.66 458.57 3700.0 3685.74 444.78\nsex*                 7 146    1.50   0.50    1.5    1.50   0.74\nyear                 8 152 2008.01   0.82 2008.0 2008.02   1.48\n                     min    max  range  skew kurtosis    se\nspecies*             1.0    1.0    0.0   NaN      NaN  0.00\nisland*              1.0    3.0    2.0 -0.09    -1.43  0.06\nbill_length_mm      32.1   46.0   13.9  0.16    -0.23  0.22\nbill_depth_mm       15.5   21.5    6.0  0.31    -0.14  0.10\nflipper_length_mm  172.0  210.0   38.0  0.09     0.24  0.53\nbody_mass_g       2850.0 4775.0 1925.0  0.28    -0.63 37.32\nsex*                 1.0    2.0    1.0  0.00    -2.01  0.04\nyear              2007.0 2009.0    2.0 -0.02    -1.53  0.07\n---------------------------------------------------- \nspecies: Chinstrap\n                  vars  n    mean     sd  median trimmed    mad\nspecies*             1 68    2.00   0.00    2.00    2.00   0.00\nisland*              2 68    2.00   0.00    2.00    2.00   0.00\nbill_length_mm       3 68   48.83   3.34   49.55   48.91   3.63\nbill_depth_mm        4 68   18.42   1.14   18.45   18.42   1.41\nflipper_length_mm    5 68  195.82   7.13  196.00  195.75   7.41\nbody_mass_g          6 68 3733.09 384.34 3700.00 3719.64 370.65\nsex*                 7 68    1.50   0.50    1.50    1.50   0.74\nyear                 8 68 2007.97   0.86 2008.00 2007.96   1.48\n                     min    max  range  skew kurtosis    se\nspecies*             2.0    2.0    0.0   NaN      NaN  0.00\nisland*              2.0    2.0    0.0   NaN      NaN  0.00\nbill_length_mm      40.9   58.0   17.1 -0.09    -0.14  0.40\nbill_depth_mm       16.4   20.8    4.4  0.01    -0.96  0.14\nflipper_length_mm  178.0  212.0   34.0 -0.01    -0.13  0.86\nbody_mass_g       2700.0 4800.0 2100.0  0.24     0.36 46.61\nsex*                 1.0    2.0    1.0  0.00    -2.03  0.06\nyear              2007.0 2009.0    2.0  0.06    -1.68  0.10\n---------------------------------------------------- \nspecies: Gentoo\n                  vars   n    mean     sd median trimmed    mad\nspecies*             1 124    3.00   0.00    3.0    3.00   0.00\nisland*              2 124    1.00   0.00    1.0    1.00   0.00\nbill_length_mm       3 123   47.50   3.08   47.3   47.38   3.11\nbill_depth_mm        4 123   14.98   0.98   15.0   14.94   1.19\nflipper_length_mm    5 123  217.19   6.48  216.0  216.83   5.93\nbody_mass_g          6 123 5076.02 504.12 5000.0 5073.48 555.98\nsex*                 7 119    1.51   0.50    2.0    1.52   0.00\nyear                 8 124 2008.08   0.79 2008.0 2008.10   1.48\n                     min    max  range  skew kurtosis    se\nspecies*             3.0    3.0    0.0   NaN      NaN  0.00\nisland*              1.0    1.0    0.0   NaN      NaN  0.00\nbill_length_mm      40.9   59.6   18.7  0.64     1.13  0.28\nbill_depth_mm       13.1   17.3    4.2  0.32    -0.65  0.09\nflipper_length_mm  203.0  231.0   28.0  0.39    -0.64  0.58\nbody_mass_g       3950.0 6300.0 2350.0  0.07    -0.78 45.45\nsex*                 1.0    2.0    1.0 -0.05    -2.01  0.05\nyear              2007.0 2009.0    2.0 -0.14    -1.41  0.07\n\npenguins %>% \n  na.omit() %>% \n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm))+\n  geom_point(aes(color = species))+\n  theme_minimal()\n\n\n\nsamp <- rchisq(600,df=3)\n\nqqplot(qchisq(ppoints(500),df=3),samp)\n\n\n\ngentoo <- penguins %>% \n  filter(species == \"Gentoo\")\n\n\n# are my numerics normal? \nnums <- rnorm(n=124,mean=47.50,sd=3.08)\n\ndescribe(nums)\n\n\n   vars   n  mean   sd median trimmed  mad   min   max range  skew\nX1    1 124 47.39 3.32  47.46   47.49 3.16 37.72 54.86 17.14 -0.26\n   kurtosis  se\nX1     0.06 0.3\n\ngentoo$bill_length_mm\n\n\n  [1] 46.1 50.0 48.7 50.0 47.6 46.5 45.4 46.7 43.3 46.8 40.9 49.0 45.5\n [14] 48.4 45.8 49.3 42.0 49.2 46.2 48.7 50.2 45.1 46.5 46.3 42.9 46.1\n [27] 44.5 47.8 48.2 50.0 47.3 42.8 45.1 59.6 49.1 48.4 42.6 44.4 44.0\n [40] 48.7 42.7 49.6 45.3 49.6 50.5 43.6 45.5 50.5 44.9 45.2 46.6 48.5\n [53] 45.1 50.1 46.5 45.0 43.8 45.5 43.2 50.4 45.3 46.2 45.7 54.3 45.8\n [66] 49.8 46.2 49.5 43.5 50.7 47.7 46.4 48.2 46.5 46.4 48.6 47.5 51.1\n [79] 45.2 45.2 49.1 52.5 47.4 50.0 44.9 50.8 43.4 51.3 47.5 52.1 47.5\n [92] 52.2 45.5 49.5 44.5 50.8 49.4 46.9 48.4 51.1 48.5 55.9 47.2 49.1\n[105] 47.3 46.8 41.7 53.4 43.3 48.1 50.5 49.8 43.5 51.5 46.2 55.1 44.5\n[118] 48.8 47.2   NA 46.8 50.4 45.2 49.9\n\ndescribe(gentoo)\n\n\n                  vars   n    mean     sd median trimmed    mad\nspecies*             1 124    3.00   0.00    3.0    3.00   0.00\nisland*              2 124    1.00   0.00    1.0    1.00   0.00\nbill_length_mm       3 123   47.50   3.08   47.3   47.38   3.11\nbill_depth_mm        4 123   14.98   0.98   15.0   14.94   1.19\nflipper_length_mm    5 123  217.19   6.48  216.0  216.83   5.93\nbody_mass_g          6 123 5076.02 504.12 5000.0 5073.48 555.98\nsex*                 7 119    1.51   0.50    2.0    1.52   0.00\nyear                 8 124 2008.08   0.79 2008.0 2008.10   1.48\n                     min    max  range  skew kurtosis    se\nspecies*             3.0    3.0    0.0   NaN      NaN  0.00\nisland*              1.0    1.0    0.0   NaN      NaN  0.00\nbill_length_mm      40.9   59.6   18.7  0.64     1.13  0.28\nbill_depth_mm       13.1   17.3    4.2  0.32    -0.65  0.09\nflipper_length_mm  203.0  231.0   28.0  0.39    -0.64  0.58\nbody_mass_g       3950.0 6300.0 2350.0  0.07    -0.78 45.45\nsex*                 1.0    2.0    1.0 -0.05    -2.01  0.05\nyear              2007.0 2009.0    2.0 -0.14    -1.41  0.07\n\nGoal of this tutorial: creating a general purpose function for\nmocking up datasets!\nOur first targeted dataset is palmers penguins, however we will then\nlook at more complex composite datasets, and randomness.\nStep 1: analyze the existing dataset- for our purposes, we will be\nmatching *insert data types that will be included here, and how we are\nmatching.\nTaking an intial look at the data, an intuitive sense may be that the\nnumbers weâ€™re seeing have something some distinctions that might be\nimportant to capture - letâ€™s try an obvious one, and try our descriptive\nstatistics grouped by species.\nAs it turns out, there are some differences (maybe not significant\nbut weâ€™ll find out) in our data! Again, letâ€™s look at our selected data\nand sample to create our distribution to create it from - Adelie\npenguins, for simplicity sake!\nAt this point we can make\nMimicing using stats functions, create as wrappers essentially.\n#General workflow- overall goal is return a dataframe? Why not at\nthis point Looping through df, understand intended distribution level to\nwork with 1. Data Type?\n\n\n\n",
    "preview": "posts/2022-06-04-creating-general-data-simulation-tools/creating-general-data-simulation-tools_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-06-04T20:38:55-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-04-functional-programming-for-data-visualization/",
    "title": "Functional Programming for Data Visualization",
    "description": "Developing a function to visualize survey data",
    "author": [
      {
        "name": "Manuel Vazquez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-06-04",
    "categories": [],
    "contents": "\nIntroduction\nThis tutorial will walk through an applied example to show how to simulate survey data and create a function to graph survey data using divergent, stacked bar charts. Creating this function would allow the user to graph survey data to any of their liking!\nSimulating data\nWe begin by simulating some survey data. We will simulate responses to 5 questions that use a likert item with a scale of one to four. To ease the simulation of the data, we will create a function to generate this data. The function has 5 formals or arguments:\nmax - An integer that represents the maximum of the likert scale. Since the items we are developing are on a 4 point scale, then we will add 4. If we wanted to simulate items on a 7 point scale, then we would add a 7 to this argument.\nn - An integer that represents the total number observations that the user wishes to generate\nweights - A list of weights that the user specifies to influence the frequency of certain responses. The length of the vector should be equal to the max.\nlevels - A numeric vector that lists the numeric number tied to each response. It should start with 1 and end with the max number specified.\nlabels - A character vector with the labels attached to the numeric values.\nThe function below also provides an error message in case certain formals are not specified correctly.\n\n\nlikert_sim <- function(max, n, weights, levels,labels) {\n  if ((length(levels) == length(labels)) & (length(weights) == max))  {\n  factor(sample(1:max, n, replace = TRUE, prob = weights),\n         levels = levels,\n         labels = labels)\n  } else {\n  stop(\"Are the length of your labels, levels, or weights equal to the likert item specified? \",  \n          \" The scale is on a \", max, \" point likert item scale. Did you add sufficient arguments?  \",\n         \", Number of weight arguments added was \",length(weights),\n         \", Number of labels arguments added was \", length(labels),\n         \", Number of level arguments added was, \", length(levels))\n  }\n}\n\n\n\nUsing this function, we simulate the data for five questions which we call q1â€¦ to q5. The sample scale we will simulate will be a 4 point agreement scale ranging from strongly disagree to strongly agree. Before running the function, we define the levels and the labels. Again, the lengths of each of these vectors should be similar.\n\n\nlevels <- c(1,2,3,4)\nlabels <- c(\"Strongly disagree\", \"Disagree\", \"Agree\", \"Strongly agree\")\n\n\n\nBelow, we use the simulate function we just created to generate 5 simulated responses with varying weights in responses. To do this, we use the levels and labels vectors defined above and plug those in to the function we defined. We define a matrix with variying weights that we will use in the function. We will use the map command to loop through the function five times, and generate a list with five items.\n\n\nallWeights <- matrix(c(.20,.30,.25,.25,\n                     .10,.50,.15,.25,\n                     .25,.10,.50,.15,\n                     .30,.30,.10,.30,\n                     .39,.45,.15,.10),\n                   nrow=5,ncol=4,byrow=TRUE)\n\n allQ <- map(1:5, ~likert_sim(4, 250, allWeights[.x,], levels, labels)) \n\n\n\nAfter we simulate this data, we bind it together into a data frame and generate a random respone ID. The data is now ready to be graphed!\n\n\nsurveydf <- reduce(allQ, cbind) \ncolnames(surveydf) <- c(\"q1\",\"q2\",\"q3\",\"q4\",\"q5\")\n\nsurveydf <- surveydf %>%\n  as_tibble() %>%\n  mutate(id = ids::random_id(250, 4))\n\n\n\nCreate a function to graph the data\nNow that we have the data simulated, we are ready to graph. In order to create a divergent horizontal bar chart, we need to take on two steps. First, we need to transform the data so that it summarizes mean responses and it is in a format that allows for graphing in bar format. This means that we need to summarize and pivot the data in a long format. After we summarize and prep the data, we move on to second step which is to actually graph the data using ggplot. We will create a function that corresponds to each of these two steps, and then apply those two functions together to graph the information.\nTo begin, we will create a function that collapses and pivots the data. The function has 4 arguments:\ndf  - specifies the data frame to be used,\ncols - is a list of the name of the columns we wish to include,\nlabels - Labels of the responses\nlevels - Numeric values of the responses\n\n\nsumm_likert <- function(df,mincol,maxcol,labels,levels) {\n  df %>%\n  pivot_longer(\n    cols = {{mincol}}:{{maxcol}},\n    names_to = \"question\",\n    values_to = \"frequency\") %>%\n  group_by(question,frequency) %>%\n    count(name = \"n_answers\") %>%\n  group_by(question) %>%\n  mutate(percent_answers = round((n_answers / sum(n_answers))*100, 0),\n         frequency = factor(frequency,\n                           levels = levels,\n                           labels = labels)) %>%\n  mutate(percent_answers = if_else(frequency == labels[1] | frequency == labels[2],\n                                   -1*percent_answers,percent_answers))\n}\n\n\n\nHere, we use the head command to check how well the function works. Note that for the last two arguments â€” labels and levels â€” I am using the vectors that were defined earlier in the data simulation step. You will notice that the percent_answers column has some negative answers. This was done on purpose so that responses that are more negative fall to the left of zero. This is needed in order to graph the likert items in a divergent bar chart.\n\n\nhead(summ_likert(surveydf,q1,q5,labels,levels))\n\n\n# A tibble: 6 Ã— 4\n# Groups:   question [2]\n  question frequency         n_answers percent_answers\n  <chr>    <fct>                 <int>           <dbl>\n1 q1       Strongly disagree        46             -18\n2 q1       Disagree                 83             -33\n3 q1       Agree                    57              23\n4 q1       Strongly agree           64              26\n5 q2       Strongly disagree        20              -8\n6 q2       Disagree                123             -49\n\nNext, we define a function to graph the data. The function will only have one argument, df. This argument refers to the data frame\n\n\ngraph_likert <- function(df) {\n  df %>%\n  ggplot(aes(x = question,\n             y = percent_answers,\n             fill = frequency)) + \n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = abs(percent_answers)),\n            position = position_stack(vjust = 0.5),\n            color = \"white\",\n            fontface = \"bold\") +\n  geom_hline(yintercept = 0) +\n  coord_flip() +\n  scale_fill_brewer(palette = \"RdYlGn\") +\n  theme_minimal() +\n  labs(fill = NULL) +\n  theme(axis.text.x = element_blank(),\n        axis.title.x = element_blank(),\n        panel.grid = element_blank(),\n        legend.position = \"top\")\n}\n\n\n\nApply the function\nFinally, we can apply the function to graph the simulated data. We can do this by using the function that pivots the data (summ_likert) and pass through the function that graphs the data (graph_likert), and that is it! If you have multiple questions, you can reuse this function and reduce the code you write!\n\n\nsumm_likert(surveydf,q1,q5,labels,levels) %>%\n  graph_likert()\n\n\n\n\nYou can also add some further customization to make the graph easier to read. For example, I can add a title and subtitle to the graph as well as meaningful x labels to the graph.\n\n\nsumm_likert(surveydf,q1,q5,labels,levels) %>%\n  graph_likert() +\n  labs(x =\" \",\n      title = \"To what extent do you agree with the following?\",\n       subtitle = \"I consider myself good at\") +\n  scale_x_discrete(labels=c(\"Teaching content to \\n\\ EL students\", \n                            \"Assessing EL students\", \n                            \"Leveraging EL student \\n\\ background in instruction\", \n                            \"Supporting English \\n\\ proficiency development\", \n                            \"Honoring EL students' \\n\\ background and culture\")) +\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\n\n",
    "preview": "posts/2022-06-04-functional-programming-for-data-visualization/functional-programming-for-data-visualization_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2022-06-06T13:22:31-07:00",
    "input_file": "functional-programming-for-data-visualization.knit.md"
  },
  {
    "path": "posts/2022-06-04-twitter-data-analysis-with-functional-programming/",
    "title": "Twitter Data Analysis with Functional Programming",
    "description": "Using R in Twitter analysis",
    "author": [
      {
        "name": "Anwesha Guha",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-06-04",
    "categories": [],
    "contents": "\nWelcome to Twitter data analysis with functional programming! Here, we will walk through how you might use functional programming methods to explore data that you might find on the web using an API. While each process looks slightly different, for this tutorial, we will focus on extracting Twitter data. While I will walk you through the code and process, here are some other links if you are interested in doing your own Twitter analysis using R and want a different setup:\nHow to Get Twitter Data Using API, R bloggers\nA Guide to Analysing Tweets with R\nSTEP 1: Get Data from Twitter API\nBefore you can work with Twitter data, you need to get data from the Twitter API. Here are the steps in R. Note: None of this code is run due to API restrictions, but you can use the code and plug in your tokens.\nFirst, set up your Dev account in Twitter. You will receive the following tokens. I have used placeholders for each of these keys for privacy.\n\n\napi_key <- \"XXXXXX\"\n\napi_key_secret <- \"XXXXX\"\n\naccess_token <- \"XXXXXX\"\n\naccess_token_secret <- \"XXXXX\"\n\nbearer_token <- \"XXXXXXXX\"\n\n\n\n\n\nlibrary(rtweet)\ntoken <- create_token(\n  app = \"r-program-project\",\n  consumer_key = api_key,\n  consumer_secret = api_key_secret,\n  access_token = access_token,\n  access_secret = access_token_secret)\n\n\n\nThen, you need to save the data you are interested in.\nI am looking at the hashtag â€œCRTâ€ or culturally relevant pedagogy. Note: the Twitter API only returns tweets from the last 6-9 days. As a result, the 18000-tweet request was not met; only 1567 tweets exist for that time window.\nIf you would like more comprehensive coverage, you can apply on the developer website â€“ though more project details will be required. I will keep the limited number for the sake of this tutorial.\n\n\ncrt_tweets <- search_tweets(\"#CRT\", \n                    n = 18000, \n                    include_rts = FALSE)\n\n\n\n\n\nwrite_csv(crt_tweets, \"~/Documents/r_projects/edld653-22/fp_collab/fp-tutorials/data/crt_tweets.csv\")\n\n\n\nSTEP 2: Data cleaning and manipulation\nIâ€™ll go ahead and load relevant libraries and the .csv file created from Step 1 here.\n\n\nlibrary(pacman)\nlibrary(readr)\nlibrary(here)\nlibrary(textdata)\np_load(httr, jsonlite, tidyverse, rtweet)\n\n\n\n\n\ncrt_tweet <- read_csv(here(\"data/crt_tweets.csv\")) \n\n\n\nSTEP 3: Data Analysis\nNow that your data is read in, we can work with it just like any other dataset in R!\nExplore source variable\nFor example, we can explore where the #CRT tweets came from. We can view these by creating a table using the source variable.\n\n\ntable(crt_tweet$source)\n\n\n\n                  AnyPoll Links                          Buffer \n                              1                               6 \n              ChurchLeaders.com                    counterganda \n                              1                             128 \n                        dlvr.it                         Echobox \n                              1                               2 \n                        Echofon                     FuturiPost2 \n                              1                               1 \n                GOGG_TwitterBot                    Hacker__News \n                             25                               1 \n                 Hootsuite Inc.                        Hypefury \n                              8                               1 \n                      Instagram                   Investor Feed \n                              7                               1 \n                     LaterMedia                        LinkedIn \n                              2                               1 \nmLabs - GestÃ£o de Redes Sociais                  rstats2twitter \n                              4                               1 \n                       Spreaker                   Sprout Social \n                              3                               2 \n                 stockosaur.com                Tweetbot for iÎŸS \n                              2                               1 \n                      TweetDeck             Twitter for Android \n                              8                             314 \n               Twitter for iPad              Twitter for iPhone \n                             45                             431 \n                Twitter for Mac            Twitter Media Studio \n                              1                               1 \n                Twitter Web App          UberSocial for Android \n                            565                               2 \n\nLetâ€™s consolidate some of these categories for better interpretability. We can call this new variable source2.\n\n\ncrt_tweet <- crt_tweet %>% \n  mutate(source2 = case_when(\n    source %in% c(\"Twitter for Android\", \"UberSocial for Android\") ~ \"Android Device\",\n    source %in% c(\"Tweetbot for iÎŸS\", \"Twitter for iPad\", \"Twitter for iPhone\", \"Twitter for Mac\") ~ \"Apple Device\",\n    source %in% c(\"Twitter Web App\") ~ \"Web App\",\n    source %in% c(\"counterganda\") ~ \"Counterganda\", #kept this separate since this was so large\n    TRUE ~ \"Other\"\n  ))\n\n\n\nHere, now we have a better variable to work with:\n\n\ntable(crt_tweet$source2)\n\n\n\nAndroid Device   Apple Device   Counterganda          Other \n           316            478            128             80 \n       Web App \n           565 \n\nSay we wanted to create visualizations for each of these groups. While there are many ways to do this, here, we will use functional programming!\nWe will focus on using these four functions:\npurrr::nest %>% mutate()\nmap()\nmap2()\nwalk()\nFirst, letâ€™s use the data we want to create the plots we want. Since we want to create visualizations using each of the tweet sources, we can group by that variable (source2) and create a nested data structure. This will create a tibble for each of the sources. From there, we can create a plot that goes through each of the sources using ggplot(). For this visualization, I chose to create frequency polygons, separated by color based on whether the tweet was a quote.\n\n\ntweet_source_plot <- crt_tweet %>% \n  group_by(source2) %>% \n  nest() %>% \n  mutate(plot = map(data, function(.x) {\n    .x %>% \n      ggplot() +\n      geom_freqpoly(aes(favourites_count, color = is_quote)) +\n      theme_classic()\n  }))\n\n\n\nNext, using map2(), we will create a large list that extracts the plot and the name of the plot, combining them into one visual.\n\n\nplots1 <- map2(tweet_source_plot$plot, tweet_source_plot$source2, ~(.x + labs(title = .y)))\n\n\n\nFinally, we print the plots! Here, we can use walk(). Since it is usually used to display side effects of functions rather than results, it is useful in this case â€“ for printing plots in a list.\n\n\nwalk(plots1, print)\n\n\n\n\nSTEP 4: Sentiment Analysis Extensions\nFor my own curiosity, and in a different vein, we can also do sentiment analyses on data extracted from Twitter (or any text data, for that matter). Using resources linked here, we can explore how mentions of #CRT might vary over time in the time, in the time segment that we extracted.\nIn order to calculate sentiments, we need to use an important library called tidytext, which allows you to access sentiment datasets and use relevant functions to get sentiment values for the tweets you have extracted.\n\n\nlibrary(tidytext)\nsentiment <- crt_tweet[,3:5] %>% unnest_tokens(output = 'word', input = 'text')\n\n\n\nWe create a long dataset using the get_sentiments() function. We then pair those with the time they were tweeted, by hour and minute.\n\n\nsentiment_dataset <- get_sentiments(\"afinn\")\nsentiment_dataset <- arrange(sentiment_dataset, -value)\n\n#merge\nsentiment <- merge(sentiment, sentiment_dataset, by = 'word')\n\n#clean\nsentiment$word <- NULL\nsentiment$screen_name <- NULL\n\n#get nearest hour of time for plot\nsentiment$hour <- format(round(sentiment$created_at, units=\"hours\"), format=\"%H:%M\")\n\n\n\nFinally, we plot the results. Viola!\n\n\nsearch_term <- \"#CRT\"\npivot <- sentiment %>%\n  group_by(hour) %>%\n  summarise(sentiment = mean(value))\n\n#plot\nggplot(pivot[-1,], aes(x = hour, y = sentiment)) + \n  geom_line(group = 1) + \n  geom_point() + \n  theme_minimal() + \n  labs(title = paste0('Average sentiment of tweetings mentioning \"',search_term,'\"'),\n       subtitle = paste0(pivot$hour[2],' - ',pivot$hour[nrow(pivot)],' on ', format(sentiment$created_at[1], '%d %B %Y')),\n       x = 'Date', \n       y = 'Sentiment', \n       caption = 'Source: Twitter API')+\n  theme(axis.text.x = element_text(angle=45))\n\n\n\n\nThanks for going through this exploratory tutorial! Good luck on your next R adventures!\n\n\n\n",
    "preview": "posts/2022-06-04-twitter-data-analysis-with-functional-programming/twitter-data-analysis-with-functional-programming_files/figure-html5/unnamed-chunk-12-1.png",
    "last_modified": "2022-06-04T20:48:24-07:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to functional programming with R.",
    "description": "Welcome to our new blog! Here, we explore different ways to work with functional programming techniques using R. We hope you enjoy reading what we have to say!",
    "author": [
      {
        "name": "Anwesha Guha, Errol Kaylor, Cassie Malcolm, Manuel Vazquez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-06-04",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2022-06-06T12:09:07-07:00",
    "input_file": {}
  }
]
