[
  {
    "path": "posts/2022-06-04-building-blocks-o-functional-programming/",
    "title": "Building Blocks of Functional Programming",
    "description": "Walk through of basic data structure explorations using map",
    "author": [
      {
        "name": "Cassie Malcolm",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-06-04",
    "categories": [],
    "contents": "\r\n\r\nHello, and welcome - I assume you are here because you want to learn\r\nmore about data science and in particular functional programming. This\r\nblog is designed to help you proceed on your data science path by\r\nproviding an introduction to the basic building blocks of functional\r\nprogramming. Weâ€™ll be working with a freely available data set on\r\nLegosÂ©, which you can find here https://www.kaggle.com/datasets/rtatman/lego-database?resource=download.\r\nLetâ€™s start by loading the following packages: purrr, tidyverse, and\r\nrepurrrsive. Take a moment to use ?package name to find out what each\r\npackage does.\r\nNext we need to set up a directory for the csv files that we will be\r\nusing.\r\n\r\n\r\nBBdata_dir <- here(\"Lego\")\r\n\r\n\r\n\r\nIâ€™ve already prepped two of the files for our use today by making\r\nsure there is an ID column that matches. We can see what the csv files\r\nare by listing them and using regexp to exclude any non-csv files.\r\n\r\n\r\nBBcsv <- fs::dir_ls(BBdata_dir, regexp = \"\\\\.csv$\")\r\n\r\nBBcsv\r\n\r\n\r\nC:/Users/errol/Documents/GitHub/fp-tutorials/fp_final_proj_acme/Lego/colors2.csv\r\nC:/Users/errol/Documents/GitHub/fp-tutorials/fp_final_proj_acme/Lego/inventory_parts.csv\r\n\r\nNext weâ€™ll use one of the base functions in R, lapply, to return a\r\nlist. The lapply function is one of the least restrictive in terms of\r\noutput within the apply family of functions. Notice that our two csv\r\nfiles are being combined into one vector based on the color_id column.\r\nThe function reduce from the purrr package allows us to join the csv\r\nfiles on a key, which in this case is â€œcolor_idâ€.\r\n\r\n\r\nBBlist <- BBcsv %>% \r\n  lapply(read_csv) %>%\r\n  reduce(full_join, by = \"color_id\")\r\n\r\nhead(BBlist)\r\n\r\n\r\n# A tibble: 6 x 9\r\n   ...1 color_id name    rgb   is_trans inventory_id part_num quantity\r\n  <dbl>    <dbl> <chr>   <chr> <lgl>           <dbl> <chr>       <dbl>\r\n1     1       -1 Unknown 0033~ FALSE              80 belvfai~        1\r\n2     1       -1 Unknown 0033~ FALSE              80 belvfem~        1\r\n3     1       -1 Unknown 0033~ FALSE              80 belvmal~        1\r\n4     1       -1 Unknown 0033~ FALSE             214 fab6e           1\r\n5     1       -1 Unknown 0033~ FALSE             250 belvfai~        1\r\n6     1       -1 Unknown 0033~ FALSE             250 belvfem~        1\r\n# ... with 1 more variable: is_spare <lgl>\r\n\r\nWe can confirm that this is a list with the typeof function. Lists\r\ncan be very helpful as they are vectors (but NOT atomic ðŸ¤¯ ones!) that\r\nallow each element to be of a different type (i.e., integer, double,\r\nlogical, character).\r\n\r\n\r\ntypeof(BBlist)\r\n\r\n\r\n[1] \"list\"\r\n\r\nIf we want to know the structure of our list we can use the below\r\nfunction to see our listâ€™s elements.\r\n\r\n\r\nstr(BBlist)\r\n\r\n\r\nspec_tbl_df [580,255 x 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\r\n $ ...1        : num [1:580255] 1 1 1 1 1 1 1 1 1 1 ...\r\n $ color_id    : num [1:580255] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\r\n $ name        : chr [1:580255] \"Unknown\" \"Unknown\" \"Unknown\" \"Unknown\" ...\r\n $ rgb         : chr [1:580255] \"0033B2\" \"0033B2\" \"0033B2\" \"0033B2\" ...\r\n $ is_trans    : logi [1:580255] FALSE FALSE FALSE FALSE FALSE FALSE ...\r\n $ inventory_id: num [1:580255] 80 80 80 214 250 250 250 250 382 382 ...\r\n $ part_num    : chr [1:580255] \"belvfair6\" \"belvfem26\" \"belvmale13\" \"fab6e\" ...\r\n $ quantity    : num [1:580255] 1 1 1 1 1 1 1 1 1 1 ...\r\n $ is_spare    : logi [1:580255] FALSE FALSE FALSE FALSE FALSE FALSE ...\r\n - attr(*, \"spec\")=\r\n  .. cols(\r\n  ..   ...1 = col_double(),\r\n  ..   color_id = col_double(),\r\n  ..   name = col_character(),\r\n  ..   rgb = col_character(),\r\n  ..   is_trans = col_logical()\r\n  .. )\r\n - attr(*, \"problems\")=<externalptr> \r\n\r\nWhat if we want to look at a particular row and column [row, column]\r\nof our list?\r\n\r\n\r\nBBlist[5, 2]\r\n\r\n\r\n# A tibble: 1 x 1\r\n  color_id\r\n     <dbl>\r\n1       -1\r\n\r\nLetâ€™s check and see if we have any missing values in our list.\r\n\r\n\r\nany(is.na(BBlist))\r\n\r\n\r\n[1] TRUE\r\n\r\nSince there are NAs in our list we can determine their position with\r\nthe following code.\r\n\r\n\r\nwhich(is.na(BBlist))\r\n\r\n\r\n [1] 3286912 3479282 3479283 3479285 3867167 4059537 4059538 4059540\r\n [9] 4447422 4639792 4639793 4639795 5027677 5220047 5220048 5220050\r\n\r\nNow letâ€™s determine an artificial mean for our data set. To do this\r\nweâ€™ll use our first functional, which is the map_chr function of the\r\npurrr package. A functional is basically a type of function whose input\r\nis also a function and whose output is a vector. In this case the\r\nfunction is mean, which will return the mean of each inventory_id. Using\r\nmap_chr ensures that the output in the new column is of type character.\r\nNote that members of the map family return a dataframe.\r\n\r\n\r\nBBdf <- BBlist %>% \r\n  mutate(average = map_chr(inventory_id, mean))\r\n\r\nhead(BBdf)\r\n\r\n\r\n# A tibble: 6 x 10\r\n   ...1 color_id name    rgb   is_trans inventory_id part_num quantity\r\n  <dbl>    <dbl> <chr>   <chr> <lgl>           <dbl> <chr>       <dbl>\r\n1     1       -1 Unknown 0033~ FALSE              80 belvfai~        1\r\n2     1       -1 Unknown 0033~ FALSE              80 belvfem~        1\r\n3     1       -1 Unknown 0033~ FALSE              80 belvmal~        1\r\n4     1       -1 Unknown 0033~ FALSE             214 fab6e           1\r\n5     1       -1 Unknown 0033~ FALSE             250 belvfai~        1\r\n6     1       -1 Unknown 0033~ FALSE             250 belvfem~        1\r\n# ... with 2 more variables: is_spare <lgl>, average <chr>\r\n\r\nHow about determining the unique amount of each variable in our\r\ndataframe? We can use another functional! Note that purrr uses less code\r\nby using the ~ symbol to take the place of function(x), which is often\r\nused with lapply, with .x as the placeholder for the thing being looped\r\nover. In this case each variable is being looped over.\r\n\r\n\r\nmap(BBdf, ~length(unique(.x)))\r\n\r\n\r\n$...1\r\n[1] 135\r\n\r\n$color_id\r\n[1] 135\r\n\r\n$name\r\n[1] 135\r\n\r\n$rgb\r\n[1] 124\r\n\r\n$is_trans\r\n[1] 2\r\n\r\n$inventory_id\r\n[1] 10725\r\n\r\n$part_num\r\n[1] 23132\r\n\r\n$quantity\r\n[1] 240\r\n\r\n$is_spare\r\n[1] 3\r\n\r\n$average\r\n[1] 10725\r\n\r\nMost of the time we have to look at individual building blocks before\r\nwe can proceed to solving larger functional programming problems. Letâ€™s\r\nstart by determining the the number of unique times each color appears.\r\nThe n column in count_colors dataframe below shows on how many different\r\nparts a color appears.\r\n\r\n\r\ncount_colors <- count(BBdf, name)\r\n\r\nhead(count_colors)\r\n\r\n\r\n# A tibble: 6 x 2\r\n  name              n\r\n  <chr>         <int>\r\n1 [No Color]     2245\r\n2 Aqua             96\r\n3 Black        115176\r\n4 Blue          29857\r\n5 Blue-Violet      67\r\n6 Bright Green   1695\r\n\r\nSecond, we can use the tally function to find the total number of\r\nparts when you consider color as a part variation. We can use this\r\nnumber later to test if the first function we are about to write is\r\nworking correctly.\r\n\r\n\r\ncount_colors %>%\r\n  tally(n)\r\n\r\n\r\n# A tibble: 1 x 1\r\n       n\r\n   <int>\r\n1 580255\r\n\r\nOur first function, super exciting! Here is an overview of the parts\r\nof the below function, which is built to return a percentage.\r\nThe functionâ€™s name is my_function Percentage\r\nis an object that is later printed with its second usage An argument\r\nx is found in the formal component of (x) The body of\r\nthe function is found between the {} The function format allows us\r\nto remove scientific notation with = F The function\r\nround allows our output to be limited to two decimal places\r\n\r\n\r\nmy_function <- function(x) {Percentage = (x/sum(x))*100; format(round(Percentage, 2), scientific = F)}\r\n\r\nmy_function(count_colors$n)\r\n\r\n\r\n  [1] \" 0.39\" \" 0.02\" \"19.85\" \" 5.15\" \" 0.01\" \" 0.29\" \" 0.06\" \" 0.34\"\r\n  [9] \" 0.05\" \" 0.25\" \" 0.60\" \" 0.00\" \" 0.00\" \" 0.00\" \" 0.13\" \" 0.00\"\r\n [17] \" 0.00\" \" 0.09\" \" 0.01\" \" 0.08\" \" 0.65\" \" 0.00\" \" 7.57\" \" 0.34\"\r\n [25] \" 0.01\" \" 1.31\" \" 0.34\" \" 0.24\" \" 0.26\" \" 0.29\" \" 0.76\" \" 0.62\"\r\n [33] \" 0.06\" \" 0.01\" \" 0.01\" \" 0.01\" \" 0.66\" \" 0.01\" \" 0.00\" \" 0.01\"\r\n [41] \" 0.00\" \" 0.00\" \" 0.00\" \" 0.01\" \" 0.00\" \" 0.01\" \" 2.04\" \" 0.06\"\r\n [49] \" 0.05\" \" 0.01\" \" 9.53\" \" 0.31\" \" 4.37\" \" 0.01\" \" 0.00\" \" 0.00\"\r\n [57] \" 0.00\" \" 0.00\" \" 0.00\" \" 0.00\" \" 0.01\" \" 0.02\" \" 0.85\" \" 0.03\"\r\n [65] \" 0.20\" \" 0.27\" \" 0.35\" \" 0.20\" \" 0.00\" \" 0.02\" \" 0.14\" \" 0.01\"\r\n [73] \" 0.03\" \" 0.00\" \" 0.01\" \" 0.04\" \" 0.00\" \" 0.14\" \" 0.02\" \" 0.12\"\r\n [81] \" 1.06\" \" 0.21\" \" 0.70\" \" 0.00\" \" 0.25\" \" 0.00\" \" 0.00\" \" 0.06\"\r\n [89] \" 0.04\" \" 8.65\" \" 2.44\" \" 0.00\" \" 0.05\" \" 0.00\" \" 0.00\" \" 0.12\"\r\n [97] \" 0.17\" \" 0.00\" \" 0.01\" \" 0.00\" \" 0.00\" \" 0.00\" \" 0.01\" \" 0.00\"\r\n[105] \" 2.36\" \" 0.38\" \" 0.00\" \" 0.08\" \" 1.32\" \" 0.45\" \" 0.14\" \" 0.34\"\r\n[113] \" 0.71\" \" 0.00\" \" 0.05\" \" 0.37\" \" 0.34\" \" 0.00\" \" 0.41\" \" 0.02\"\r\n[121] \" 0.07\" \" 0.99\" \" 0.01\" \" 0.57\" \" 0.00\" \" 0.00\" \" 0.00\" \" 0.02\"\r\n[129] \" 0.02\" \" 0.00\" \" 0.00\" \" 0.01\" \"11.47\" \" 6.69\" \" 0.03\"\r\n\r\nIf we want to check that our function is working correctly we can use\r\na data point (letâ€™s check the second data point for color Aqua) and\r\nmanually determine the corresponding percentage.\r\n\r\n\r\n(96/580255)*100\r\n\r\n\r\n[1] 0.01654445\r\n\r\nOur function worked! The above number when rounded is 0.02, which is\r\nthe second percentage.\r\nThat wraps up our introduction to functional programming! In the next\r\nparts of the blog youâ€™ll learn more about functions and how to perform\r\nmore complex functional programming.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-06-06T22:01:50-07:00",
    "input_file": "building-blocks-o-functional-programming.knit.md"
  },
  {
    "path": "posts/2022-06-04-creating-general-data-simulation-tools/",
    "title": "Creating General Data Simulation Tools",
    "description": "Create your own functions to easily simulate datasets!",
    "author": [
      {
        "name": "Errol Kaylor",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-06-04",
    "categories": [],
    "contents": "\r\nOutline\r\nOverall goal: demonstrate ways to generate datasets for classroom\r\nuse!\r\nPart 1: Mocking data based on existing data set (of\r\nnumerical/character data) - Load in data, and produce descriptive stats.\r\n- Create a list of the columns/data types that are you are looking to\r\nmock up. - Populate the list with mocked data, depending on the types. -\r\nLoop through the process, depending on # of data sets needed. -\r\nDemonstrate function that walks them through the process â€“ Simulating\r\ndifferent types of data! - Mocking numerical data â€“ Distribution\r\nfunctions, lists - Mocking character data â€“ Use of stringi for character\r\nstrings â€“ Generate strings of specific length/format - Mocking logical\r\ndata â€“ Mock survey data\r\n\r\n\r\nlibrary(palmerpenguins)\r\nlibrary(stats)\r\nlibrary(psych)\r\nlibrary(ggplot2)\r\nlibrary(tidyverse)\r\npenguins\r\n\r\n\r\n# A tibble: 344 x 8\r\n   species island    bill_length_mm bill_depth_mm flipper_length_mm\r\n   <fct>   <fct>              <dbl>         <dbl>             <int>\r\n 1 Adelie  Torgersen           39.1          18.7               181\r\n 2 Adelie  Torgersen           39.5          17.4               186\r\n 3 Adelie  Torgersen           40.3          18                 195\r\n 4 Adelie  Torgersen           NA            NA                  NA\r\n 5 Adelie  Torgersen           36.7          19.3               193\r\n 6 Adelie  Torgersen           39.3          20.6               190\r\n 7 Adelie  Torgersen           38.9          17.8               181\r\n 8 Adelie  Torgersen           39.2          19.6               195\r\n 9 Adelie  Torgersen           34.1          18.1               193\r\n10 Adelie  Torgersen           42            20.2               190\r\n# ... with 334 more rows, and 3 more variables: body_mass_g <int>,\r\n#   sex <fct>, year <int>\r\n\r\n#first function, tell us descriptives, and show us some boxplots?\r\nstr(penguins)\r\n\r\n\r\ntibble [344 x 8] (S3: tbl_df/tbl/data.frame)\r\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\r\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\r\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\r\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\r\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\r\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\r\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\r\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\r\n\r\nspecies_tables <- describeBy(penguins,group=\"species\")\r\n\r\nspecies_tables\r\n\r\n\r\n\r\n Descriptive statistics by group \r\nspecies: Adelie\r\n                  vars   n    mean     sd median trimmed    mad\r\nspecies*             1 152    1.00   0.00    1.0    1.00   0.00\r\nisland*              2 152    2.05   0.80    2.0    2.07   1.48\r\nbill_length_mm       3 151   38.79   2.66   38.8   38.75   2.97\r\nbill_depth_mm        4 151   18.35   1.22   18.4   18.29   1.19\r\nflipper_length_mm    5 151  189.95   6.54  190.0  189.93   7.41\r\nbody_mass_g          6 151 3700.66 458.57 3700.0 3685.74 444.78\r\nsex*                 7 146    1.50   0.50    1.5    1.50   0.74\r\nyear                 8 152 2008.01   0.82 2008.0 2008.02   1.48\r\n                     min    max  range  skew kurtosis    se\r\nspecies*             1.0    1.0    0.0   NaN      NaN  0.00\r\nisland*              1.0    3.0    2.0 -0.09    -1.43  0.06\r\nbill_length_mm      32.1   46.0   13.9  0.16    -0.23  0.22\r\nbill_depth_mm       15.5   21.5    6.0  0.31    -0.14  0.10\r\nflipper_length_mm  172.0  210.0   38.0  0.09     0.24  0.53\r\nbody_mass_g       2850.0 4775.0 1925.0  0.28    -0.63 37.32\r\nsex*                 1.0    2.0    1.0  0.00    -2.01  0.04\r\nyear              2007.0 2009.0    2.0 -0.02    -1.53  0.07\r\n---------------------------------------------------- \r\nspecies: Chinstrap\r\n                  vars  n    mean     sd  median trimmed    mad\r\nspecies*             1 68    2.00   0.00    2.00    2.00   0.00\r\nisland*              2 68    2.00   0.00    2.00    2.00   0.00\r\nbill_length_mm       3 68   48.83   3.34   49.55   48.91   3.63\r\nbill_depth_mm        4 68   18.42   1.14   18.45   18.42   1.41\r\nflipper_length_mm    5 68  195.82   7.13  196.00  195.75   7.41\r\nbody_mass_g          6 68 3733.09 384.34 3700.00 3719.64 370.65\r\nsex*                 7 68    1.50   0.50    1.50    1.50   0.74\r\nyear                 8 68 2007.97   0.86 2008.00 2007.96   1.48\r\n                     min    max  range  skew kurtosis    se\r\nspecies*             2.0    2.0    0.0   NaN      NaN  0.00\r\nisland*              2.0    2.0    0.0   NaN      NaN  0.00\r\nbill_length_mm      40.9   58.0   17.1 -0.09    -0.14  0.40\r\nbill_depth_mm       16.4   20.8    4.4  0.01    -0.96  0.14\r\nflipper_length_mm  178.0  212.0   34.0 -0.01    -0.13  0.86\r\nbody_mass_g       2700.0 4800.0 2100.0  0.24     0.36 46.61\r\nsex*                 1.0    2.0    1.0  0.00    -2.03  0.06\r\nyear              2007.0 2009.0    2.0  0.06    -1.68  0.10\r\n---------------------------------------------------- \r\nspecies: Gentoo\r\n                  vars   n    mean     sd median trimmed    mad\r\nspecies*             1 124    3.00   0.00    3.0    3.00   0.00\r\nisland*              2 124    1.00   0.00    1.0    1.00   0.00\r\nbill_length_mm       3 123   47.50   3.08   47.3   47.38   3.11\r\nbill_depth_mm        4 123   14.98   0.98   15.0   14.94   1.19\r\nflipper_length_mm    5 123  217.19   6.48  216.0  216.83   5.93\r\nbody_mass_g          6 123 5076.02 504.12 5000.0 5073.48 555.98\r\nsex*                 7 119    1.51   0.50    2.0    1.52   0.00\r\nyear                 8 124 2008.08   0.79 2008.0 2008.10   1.48\r\n                     min    max  range  skew kurtosis    se\r\nspecies*             3.0    3.0    0.0   NaN      NaN  0.00\r\nisland*              1.0    1.0    0.0   NaN      NaN  0.00\r\nbill_length_mm      40.9   59.6   18.7  0.64     1.13  0.28\r\nbill_depth_mm       13.1   17.3    4.2  0.32    -0.65  0.09\r\nflipper_length_mm  203.0  231.0   28.0  0.39    -0.64  0.58\r\nbody_mass_g       3950.0 6300.0 2350.0  0.07    -0.78 45.45\r\nsex*                 1.0    2.0    1.0 -0.05    -2.01  0.05\r\nyear              2007.0 2009.0    2.0 -0.14    -1.41  0.07\r\n\r\npenguins %>% \r\n  na.omit() %>% \r\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm))+\r\n  geom_point(aes(color = species))+\r\n  theme_minimal()\r\n\r\n\r\n\r\nsamp <- rchisq(600,df=3)\r\n\r\nqqplot(qchisq(ppoints(500),df=3),samp)\r\n\r\n\r\n\r\ngentoo <- penguins %>% \r\n  filter(species == \"Gentoo\")\r\n\r\n\r\n# are my numerics normal? \r\nnums <- rnorm(n=124,mean=47.50,sd=3.08)\r\n\r\ndescribe(nums)\r\n\r\n\r\n   vars   n  mean   sd median trimmed  mad   min   max range skew\r\nX1    1 124 47.63 2.95   47.7   47.54 2.89 40.99 56.19  15.2 0.24\r\n   kurtosis   se\r\nX1    -0.16 0.27\r\n\r\ngentoo$bill_length_mm\r\n\r\n\r\n  [1] 46.1 50.0 48.7 50.0 47.6 46.5 45.4 46.7 43.3 46.8 40.9 49.0 45.5\r\n [14] 48.4 45.8 49.3 42.0 49.2 46.2 48.7 50.2 45.1 46.5 46.3 42.9 46.1\r\n [27] 44.5 47.8 48.2 50.0 47.3 42.8 45.1 59.6 49.1 48.4 42.6 44.4 44.0\r\n [40] 48.7 42.7 49.6 45.3 49.6 50.5 43.6 45.5 50.5 44.9 45.2 46.6 48.5\r\n [53] 45.1 50.1 46.5 45.0 43.8 45.5 43.2 50.4 45.3 46.2 45.7 54.3 45.8\r\n [66] 49.8 46.2 49.5 43.5 50.7 47.7 46.4 48.2 46.5 46.4 48.6 47.5 51.1\r\n [79] 45.2 45.2 49.1 52.5 47.4 50.0 44.9 50.8 43.4 51.3 47.5 52.1 47.5\r\n [92] 52.2 45.5 49.5 44.5 50.8 49.4 46.9 48.4 51.1 48.5 55.9 47.2 49.1\r\n[105] 47.3 46.8 41.7 53.4 43.3 48.1 50.5 49.8 43.5 51.5 46.2 55.1 44.5\r\n[118] 48.8 47.2   NA 46.8 50.4 45.2 49.9\r\n\r\ndescribe(gentoo)\r\n\r\n\r\n                  vars   n    mean     sd median trimmed    mad\r\nspecies*             1 124    3.00   0.00    3.0    3.00   0.00\r\nisland*              2 124    1.00   0.00    1.0    1.00   0.00\r\nbill_length_mm       3 123   47.50   3.08   47.3   47.38   3.11\r\nbill_depth_mm        4 123   14.98   0.98   15.0   14.94   1.19\r\nflipper_length_mm    5 123  217.19   6.48  216.0  216.83   5.93\r\nbody_mass_g          6 123 5076.02 504.12 5000.0 5073.48 555.98\r\nsex*                 7 119    1.51   0.50    2.0    1.52   0.00\r\nyear                 8 124 2008.08   0.79 2008.0 2008.10   1.48\r\n                     min    max  range  skew kurtosis    se\r\nspecies*             3.0    3.0    0.0   NaN      NaN  0.00\r\nisland*              1.0    1.0    0.0   NaN      NaN  0.00\r\nbill_length_mm      40.9   59.6   18.7  0.64     1.13  0.28\r\nbill_depth_mm       13.1   17.3    4.2  0.32    -0.65  0.09\r\nflipper_length_mm  203.0  231.0   28.0  0.39    -0.64  0.58\r\nbody_mass_g       3950.0 6300.0 2350.0  0.07    -0.78 45.45\r\nsex*                 1.0    2.0    1.0 -0.05    -2.01  0.05\r\nyear              2007.0 2009.0    2.0 -0.14    -1.41  0.07\r\n\r\nGoal of this tutorial: creating a general purpose function for\r\nmocking up datasets!\r\nOur first targeted dataset is palmers penguins, however we will then\r\nlook at more complex composite datasets, and randomness.\r\nStep 1: analyze the existing dataset- for our purposes, we will be\r\nmatching *insert data types that will be included here, and how we are\r\nmatching.\r\nTaking an intial look at the data, an intuitive sense may be that the\r\nnumbers weâ€™re seeing have something some distinctions that might be\r\nimportant to capture - letâ€™s try an obvious one, and try our descriptive\r\nstatistics grouped by species.\r\nAs it turns out, there are some differences (maybe not significant\r\nbut weâ€™ll find out) in our data! Again, letâ€™s look at our selected data\r\nand sample to create our distribution to create it from - Adelie\r\npenguins, for simplicity sake!\r\nAt this point we can make\r\nMimicing using stats functions, create as wrappers essentially.\r\n#General workflow- overall goal is return a dataframe? Why not at\r\nthis point Looping through df, understand intended distribution level to\r\nwork with 1. Data Type?\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-06-04-creating-general-data-simulation-tools/creating-general-data-simulation-tools_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-06-06T21:54:33-07:00",
    "input_file": "creating-general-data-simulation-tools.knit.md"
  },
  {
    "path": "posts/2022-06-04-functional-programming-for-data-visualization/",
    "title": "Functional Programming for Data Visualization",
    "description": "Developing a function to visualize survey data",
    "author": [
      {
        "name": "Manuel Vazquez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-06-04",
    "categories": [],
    "contents": "\nIntroduction\nThis tutorial will walk through an applied example to show how to simulate survey data and create a function to graph survey data using divergent, stacked bar charts. Creating this function would allow the user to graph survey data to any of their liking!\nSimulating data\nWe begin by simulating some survey data. We will simulate responses to 5 questions that use a likert item with a scale of one to four. To ease the simulation of the data, we will create a function to generate this data. The function has 5 formals or arguments:\nmax - An integer that represents the maximum of the likert scale. Since the items we are developing are on a 4 point scale, then we will add 4. If we wanted to simulate items on a 7 point scale, then we would add a 7 to this argument.\nn - An integer that represents the total number observations that the user wishes to generate\nweights - A list of weights that the user specifies to influence the frequency of certain responses. The length of the vector should be equal to the max.\nlevels - A numeric vector that lists the numeric number tied to each response. It should start with 1 and end with the max number specified.\nlabels - A character vector with the labels attached to the numeric values.\nThe function below also provides an error message in case certain formals are not specified correctly.\n\n\nlikert_sim <- function(max, n, weights, levels,labels) {\n  if ((length(levels) == length(labels)) & (length(weights) == max))  {\n  factor(sample(1:max, n, replace = TRUE, prob = weights),\n         levels = levels,\n         labels = labels)\n  } else {\n  stop(\"Are the length of your labels, levels, or weights equal to the likert item specified? \",  \n          \" The scale is on a \", max, \" point likert item scale. Did you add sufficient arguments?  \",\n         \", Number of weight arguments added was \",length(weights),\n         \", Number of labels arguments added was \", length(labels),\n         \", Number of level arguments added was, \", length(levels))\n  }\n}\n\n\n\nUsing this function, we simulate the data for five questions which we call q1â€¦ to q5. The sample scale we will simulate will be a 4 point agreement scale ranging from strongly disagree to strongly agree. Before running the function, we define the levels and the labels. Again, the lengths of each of these vectors should be similar.\n\n\nlevels <- c(1,2,3,4)\nlabels <- c(\"Strongly disagree\", \"Disagree\", \"Agree\", \"Strongly agree\")\n\n\n\nBelow, we use the simulate function we just created to generate 5 simulated responses with varying weights in responses. To do this, we use the levels and labels vectors defined above and plug those in to the function we defined. We define a matrix with variying weights that we will use in the function. We will use the map command to loop through the function five times, and generate a list with five items.\n\n\nallWeights <- matrix(c(.20,.30,.25,.25,\n                     .10,.50,.15,.25,\n                     .25,.10,.50,.15,\n                     .30,.30,.10,.30,\n                     .39,.45,.15,.10),\n                   nrow=5,ncol=4,byrow=TRUE)\n\n allQ <- map(1:5, ~likert_sim(4, 250, allWeights[.x,], levels, labels)) \n\n\n\nAfter we simulate this data, we bind it together into a data frame and generate a random respone ID. The data is now ready to be graphed!\n\n\nsurveydf <- reduce(allQ, cbind) \ncolnames(surveydf) <- c(\"q1\",\"q2\",\"q3\",\"q4\",\"q5\")\n\nsurveydf <- surveydf %>%\n  as_tibble() %>%\n  mutate(id = ids::random_id(250, 4))\n\n\n\nCreate a function to graph the data\nNow that we have the data simulated, we are ready to graph. In order to create a divergent horizontal bar chart, we need to take on two steps. First, we need to transform the data so that it summarizes mean responses and it is in a format that allows for graphing in bar format. This means that we need to summarize and pivot the data in a long format. After we summarize and prep the data, we move on to second step which is to actually graph the data using ggplot. We will create a function that corresponds to each of these two steps, and then apply those two functions together to graph the information.\nTo begin, we will create a function that collapses and pivots the data. The function has 4 arguments:\ndf  - specifies the data frame to be used,\ncols - is a list of the name of the columns we wish to include,\nlabels - Labels of the responses\nlevels - Numeric values of the responses\n\n\nsumm_likert <- function(df,mincol,maxcol,labels,levels) {\n  df %>%\n  pivot_longer(\n    cols = {{mincol}}:{{maxcol}},\n    names_to = \"question\",\n    values_to = \"frequency\") %>%\n  group_by(question,frequency) %>%\n    count(name = \"n_answers\") %>%\n  group_by(question) %>%\n  mutate(percent_answers = round((n_answers / sum(n_answers))*100, 0),\n         frequency = factor(frequency,\n                           levels = levels,\n                           labels = labels)) %>%\n  mutate(percent_answers = if_else(frequency == labels[1] | frequency == labels[2],\n                                   -1*percent_answers,percent_answers))\n}\n\n\n\nHere, we use the head command to check how well the function works. Note that for the last two arguments â€” labels and levels â€” I am using the vectors that were defined earlier in the data simulation step. You will notice that the percent_answers column has some negative answers. This was done on purpose so that responses that are more negative fall to the left of zero. This is needed in order to graph the likert items in a divergent bar chart.\n\n\nhead(summ_likert(surveydf,q1,q5,labels,levels))\n\n\n# A tibble: 6 Ã— 4\n# Groups:   question [2]\n  question frequency         n_answers percent_answers\n  <chr>    <fct>                 <int>           <dbl>\n1 q1       Strongly disagree        46             -18\n2 q1       Disagree                 83             -33\n3 q1       Agree                    57              23\n4 q1       Strongly agree           64              26\n5 q2       Strongly disagree        20              -8\n6 q2       Disagree                123             -49\n\nNext, we define a function to graph the data. The function will only have one argument, df. This argument refers to the data frame\n\n\ngraph_likert <- function(df) {\n  df %>%\n  ggplot(aes(x = question,\n             y = percent_answers,\n             fill = frequency)) + \n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = abs(percent_answers)),\n            position = position_stack(vjust = 0.5),\n            color = \"white\",\n            fontface = \"bold\") +\n  geom_hline(yintercept = 0) +\n  coord_flip() +\n  scale_fill_brewer(palette = \"RdYlGn\") +\n  theme_minimal() +\n  labs(fill = NULL) +\n  theme(axis.text.x = element_blank(),\n        axis.title.x = element_blank(),\n        panel.grid = element_blank(),\n        legend.position = \"top\")\n}\n\n\n\nApply the function\nFinally, we can apply the function to graph the simulated data. We can do this by using the function that pivots the data (summ_likert) and pass through the function that graphs the data (graph_likert), and that is it! If you have multiple questions, you can reuse this function and reduce the code you write!\n\n\nsumm_likert(surveydf,q1,q5,labels,levels) %>%\n  graph_likert()\n\n\n\n\nYou can also add some further customization to make the graph easier to read. For example, I can add a title and subtitle to the graph as well as meaningful x labels to the graph.\n\n\nsumm_likert(surveydf,q1,q5,labels,levels) %>%\n  graph_likert() +\n  labs(x =\" \",\n      title = \"To what extent do you agree with the following?\",\n       subtitle = \"I consider myself good at\") +\n  scale_x_discrete(labels=c(\"Teaching content to \\n\\ EL students\", \n                            \"Assessing EL students\", \n                            \"Leveraging EL student \\n\\ background in instruction\", \n                            \"Supporting English \\n\\ proficiency development\", \n                            \"Honoring EL students' \\n\\ background and culture\")) +\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\n\n",
    "preview": "posts/2022-06-04-functional-programming-for-data-visualization/functional-programming-for-data-visualization_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2022-06-06T13:22:31-07:00",
    "input_file": "functional-programming-for-data-visualization.knit.md"
  },
  {
    "path": "posts/2022-06-04-twitter-data-analysis-with-functional-programming/",
    "title": "Twitter Data Analysis with Functional Programming",
    "description": "Using R in Twitter analysis",
    "author": [
      {
        "name": "Anwesha Guha",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-06-04",
    "categories": [],
    "contents": "\nWelcome to Twitter data analysis with functional programming! Here, we will walk through how you might use functional programming methods to explore data that you might find on the web using an API. While each process looks slightly different, for this tutorial, we will focus on extracting Twitter data. While I will walk you through the code and process, here are some other links if you are interested in doing your own Twitter analysis using R and want a different setup:\nHow to Get Twitter Data Using API, R bloggers\nA Guide to Analysing Tweets with R\nSTEP 1: Get Data from Twitter API\nBefore you can work with Twitter data, you need to get data from the Twitter API. Here are the steps in R. Note: None of this code is run due to API restrictions, but you can use the code and plug in your tokens.\nFirst, set up your Dev account in Twitter. You will receive the following tokens. I have used placeholders for each of these keys for privacy.\n\n\napi_key <- \"XXXXXX\"\n\napi_key_secret <- \"XXXXX\"\n\naccess_token <- \"XXXXXX\"\n\naccess_token_secret <- \"XXXXX\"\n\nbearer_token <- \"XXXXXXXX\"\n\n\n\n\n\nlibrary(rtweet)\ntoken <- create_token(\n  app = \"r-program-project\",\n  consumer_key = api_key,\n  consumer_secret = api_key_secret,\n  access_token = access_token,\n  access_secret = access_token_secret)\n\n\n\nThen, you need to save the data you are interested in.\nI am looking at the hashtag â€œCRTâ€ or culturally relevant pedagogy. Note: the Twitter API only returns tweets from the last 6-9 days. As a result, the 18000-tweet request was not met; only 1567 tweets exist for that time window.\nIf you would like more comprehensive coverage, you can apply on the developer website â€“ though more project details will be required. I will keep the limited number for the sake of this tutorial.\n\n\ncrt_tweets <- search_tweets(\"#CRT\", \n                    n = 18000, \n                    include_rts = FALSE)\n\n\n\n\n\nwrite_csv(crt_tweets, \"~/Documents/r_projects/edld653-22/fp_collab/fp-tutorials/data/crt_tweets.csv\")\n\n\n\nSTEP 2: Data cleaning and manipulation\nIâ€™ll go ahead and load relevant libraries and the .csv file created from Step 1 here.\n\n\nlibrary(pacman)\nlibrary(readr)\nlibrary(here)\nlibrary(textdata)\np_load(httr, jsonlite, tidyverse, rtweet)\n\n\n\n\n\ncrt_tweet <- read_csv(here(\"data/crt_tweets.csv\")) \n\n\n\nSTEP 3: Data Analysis\nNow that your data is read in, we can work with it just like any other dataset in R!\nExplore source variable\nFor example, we can explore where the #CRT tweets came from. We can view these by creating a table using the source variable.\n\n\ntable(crt_tweet$source)\n\n\n\n                  AnyPoll Links                          Buffer \n                              1                               6 \n              ChurchLeaders.com                    counterganda \n                              1                             128 \n                        dlvr.it                         Echobox \n                              1                               2 \n                        Echofon                     FuturiPost2 \n                              1                               1 \n                GOGG_TwitterBot                    Hacker__News \n                             25                               1 \n                 Hootsuite Inc.                        Hypefury \n                              8                               1 \n                      Instagram                   Investor Feed \n                              7                               1 \n                     LaterMedia                        LinkedIn \n                              2                               1 \nmLabs - GestÃ£o de Redes Sociais                  rstats2twitter \n                              4                               1 \n                       Spreaker                   Sprout Social \n                              3                               2 \n                 stockosaur.com                Tweetbot for iÎŸS \n                              2                               1 \n                      TweetDeck             Twitter for Android \n                              8                             314 \n               Twitter for iPad              Twitter for iPhone \n                             45                             431 \n                Twitter for Mac            Twitter Media Studio \n                              1                               1 \n                Twitter Web App          UberSocial for Android \n                            565                               2 \n\nLetâ€™s consolidate some of these categories for better interpretability. We can call this new variable source2.\n\n\ncrt_tweet <- crt_tweet %>% \n  mutate(source2 = case_when(\n    source %in% c(\"Twitter for Android\", \"UberSocial for Android\") ~ \"Android Device\",\n    source %in% c(\"Tweetbot for iÎŸS\", \"Twitter for iPad\", \"Twitter for iPhone\", \"Twitter for Mac\") ~ \"Apple Device\",\n    source %in% c(\"Twitter Web App\") ~ \"Web App\",\n    source %in% c(\"counterganda\") ~ \"Counterganda\", #kept this separate since this was so large\n    TRUE ~ \"Other\"\n  ))\n\n\n\nHere, now we have a better variable to work with:\n\n\ntable(crt_tweet$source2)\n\n\n\nAndroid Device   Apple Device   Counterganda          Other \n           316            478            128             80 \n       Web App \n           565 \n\nSay we wanted to create visualizations for each of these groups. While there are many ways to do this, here, we will use functional programming!\nWe will focus on using these four functions:\npurrr::nest %>% mutate()\nmap()\nmap2()\nwalk()\nFirst, letâ€™s use the data we want to create the plots we want. Since we want to create visualizations using each of the tweet sources, we can group by that variable (source2) and create a nested data structure. This will create a tibble for each of the sources. From there, we can create a plot that goes through each of the sources using ggplot(). For this visualization, I chose to create frequency polygons, separated by color based on whether the tweet was a quote.\n\n\ntweet_source_plot <- crt_tweet %>% \n  group_by(source2) %>% \n  nest() %>% \n  mutate(plot = map(data, function(.x) {\n    .x %>% \n      ggplot() +\n      geom_freqpoly(aes(favourites_count, color = is_quote)) +\n      theme_classic()\n  }))\n\n\n\nNext, using map2(), we will create a large list that extracts the plot and the name of the plot, combining them into one visual.\n\n\nplots1 <- map2(tweet_source_plot$plot, tweet_source_plot$source2, ~(.x + labs(title = .y)))\n\n\n\nFinally, we print the plots! Here, we can use walk(). Since it is usually used to display side effects of functions rather than results, it is useful in this case â€“ for printing plots in a list.\n\n\nwalk(plots1, print)\n\n\n\n\nSTEP 4: Sentiment Analysis Extensions\nFor my own curiosity, and in a different vein, we can also do sentiment analyses on data extracted from Twitter (or any text data, for that matter). Using resources linked here, we can explore how mentions of #CRT might vary over time in the time, in the time segment that we extracted.\nIn order to calculate sentiments, we need to use an important library called tidytext, which allows you to access sentiment datasets and use relevant functions to get sentiment values for the tweets you have extracted.\n\n\nlibrary(tidytext)\nsentiment <- crt_tweet[,3:5] %>% unnest_tokens(output = 'word', input = 'text')\n\n\n\nWe create a long dataset using the get_sentiments() function. We then pair those with the time they were tweeted, by hour and minute.\n\n\nsentiment_dataset <- get_sentiments(\"afinn\")\nsentiment_dataset <- arrange(sentiment_dataset, -value)\n\n#merge\nsentiment <- merge(sentiment, sentiment_dataset, by = 'word')\n\n#clean\nsentiment$word <- NULL\nsentiment$screen_name <- NULL\n\n#get nearest hour of time for plot\nsentiment$hour <- format(round(sentiment$created_at, units=\"hours\"), format=\"%H:%M\")\n\n\n\nFinally, we plot the results. Viola!\n\n\nsearch_term <- \"#CRT\"\npivot <- sentiment %>%\n  group_by(hour) %>%\n  summarise(sentiment = mean(value))\n\n#plot\nggplot(pivot[-1,], aes(x = hour, y = sentiment)) + \n  geom_line(group = 1) + \n  geom_point() + \n  theme_minimal() + \n  labs(title = paste0('Average sentiment of tweetings mentioning \"',search_term,'\"'),\n       subtitle = paste0(pivot$hour[2],' - ',pivot$hour[nrow(pivot)],' on ', format(sentiment$created_at[1], '%d %B %Y')),\n       x = 'Date', \n       y = 'Sentiment', \n       caption = 'Source: Twitter API')+\n  theme(axis.text.x = element_text(angle=45))\n\n\n\n\nThanks for going through this exploratory tutorial! Good luck on your next R adventures!\n\n\n\n",
    "preview": "posts/2022-06-04-twitter-data-analysis-with-functional-programming/twitter-data-analysis-with-functional-programming_files/figure-html5/unnamed-chunk-12-1.png",
    "last_modified": "2022-06-04T20:48:24-07:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to functional programming with R.",
    "description": "Welcome to our new blog! Here, we explore different ways to work with functional programming techniques using R. We hope you enjoy reading what we have to say!",
    "author": [
      {
        "name": "Anwesha Guha, Errol Kaylor, Cassie Malcolm, Manuel Vazquez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-06-04",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2022-06-06T12:09:07-07:00",
    "input_file": {}
  }
]
